<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://kgourgou.me/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kgourgou.me/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-11T09:31:09+00:00</updated><id>https://kgourgou.me/feed.xml</id><title type="html">Kostis Gourgoulias</title><subtitle></subtitle><entry><title type="html">Presented Our Work On Unlearning At The U&amp;amp;me Workshop At Eccv 2024</title><link href="https://kgourgou.me/blog/2024/Presented-our-work-on-unlearning-at-the-U&Me-workshop-at-ECCV-2024/" rel="alternate" type="text/html" title="Presented Our Work On Unlearning At The U&amp;amp;me Workshop At Eccv 2024"/><published>2024-10-11T00:00:00+00:00</published><updated>2024-10-11T00:00:00+00:00</updated><id>https://kgourgou.me/blog/2024/Presented-our-work-on-unlearning-at-the-U&amp;Me-workshop-at-ECCV-2024</id><content type="html" xml:base="https://kgourgou.me/blog/2024/Presented-our-work-on-unlearning-at-the-U&amp;Me-workshop-at-ECCV-2024/"><![CDATA[<hr/> <p>layout: distill title: “Presented our work on unlearning at the U&amp;Me workshop at ECCV 2024” date: 2024-10-11 10-26-03 +0100 category: tags: —</p> <p>I presented our work on unlearning at the <a href="https://sites.google.com/view/u-and-me-workshop">U&amp;Me workshop</a> at ECCV 2024. The workshop was a great opportunity to discuss the latest research on unlearning and to meet other researchers working on similar topics.</p> <p>Frankly the highlight was the invited talks by David Bau and Rita Cucchiara. <a href="https://sites.google.com/view/u-and-me-workshop/schedule">They both shared their slides</a>, so there’s no reason for me post pictures here.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">‘Yet another set of notes on self-attention’</title><link href="https://kgourgou.me/blog/2024/Yet-another-set-of-notes-on-self-attention/" rel="alternate" type="text/html" title="‘Yet another set of notes on self-attention’"/><published>2024-06-27T00:00:00+00:00</published><updated>2024-06-27T00:00:00+00:00</updated><id>https://kgourgou.me/blog/2024/Yet-another-set-of-notes-on-self-attention</id><content type="html" xml:base="https://kgourgou.me/blog/2024/Yet-another-set-of-notes-on-self-attention/"><![CDATA[<p>This is a living document that I’ll extend with more information as I go. As attention is dead-center when it comes to LLMs, it’s not weird that a lot has been written about it. Here I wanted to collect some of my own questions with their answers (for easy reference).</p> <p>Parts of these notes are based on the new book “Deep Learning” by Bishop &amp; Bishop<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">1</a></sup>, which I recommend. They also have some comments from me (and, of course, all mistakes / typos are my own).</p> <pre><code class="language-toc">
</code></pre> <h1 id="dot-product-self-attention">Dot-product self-attention</h1> <p>Let’s get started. We have a matrix \(X\in\mathbb{R}^{n\times m}\) which represents a sequence of \(n\) \(m\)-dimensional vectors. For the language modeling example, each one of those could be the embedding of a single token.</p> <p>Let’s suppose that we are in the business of modeling, so we would like to map \(X\) to a \(Y\in \mathbb{R}^{n\times m}\) such that each m-dimensional vector in \(Y\) contains information from all \(X_j\) (\(X_j\) being the \(j\)-th column vector). Perhaps the simplest way is \(Y_i=\sum_jA_{ij}X_j,\) where we assume that \(A_{ij}\in [0,1]\) for all \(i,j\). Restricting \(A_{ij}\) such that \(\sum_{j}A_{ij}=1\) for all \(i\) has some nice properties, we can now think of \(Y_i\) as a weighted mean of the \(X_j\), and we just get to decide how much of each \(X_j\) to use.</p> <p>Following this recipe further, we can pick \(A_{ij}\) according to how relevant each \(X_j\)​ is to every other. One way to capture relevance is through similarity, leading to “<em>dot-product self-attention</em>”<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>:</p> <p>\(A_{ij}=\text{softmax}(XX^T)_{ij}=\frac{\exp(X_i^TX_j)}{\sum_{k}\exp(X_i^TX_k)}.\)​</p> <p>As \(X\in\mathbb{R}^{n\times m}\), \(XX^T\) has dimensions \(n \times n\)​, i.e., it is quadratic on sequence size.</p> <p>To get \(Y\), we can just do \(Y=\text{softmax}(XX^T)X.\)​</p> <p>This is fine, but:</p> <ol> <li>there’s nothing learnable here (how do we know that the raw \(X\) is in the right representation to get the best possible \(Y\) for our task?) and</li> <li>every dimension of an \(X_i\) gets the same weight.</li> </ol> <p>We can address these points by introducing a new matrix, \(U\in\mathbb{R}^{D\times D}\), with learnable parameters such that \(\tilde{X} = XU\), and so</p> \[Y=\text{softmax}(\tilde{X}\tilde{X}^T)\tilde{X}=\text{softmax}(XUU^TX^T)XU.\] <p>Progress, but now \(\tilde{X}\tilde{X}^T\) is always a symmetric matrix regardless of \(U\), so we cannot capture asymmetric relationships. This motivates using different parameters for the parts of the attention matrix and the final mapping:</p> \[\begin{align} Q&amp;=XW_Q,\ W_Q\in\mathbb{R}^{m\times D_K},\\ K&amp;=XW_k,\ W_K\in\mathbb{R}^{m\times D_K},\\ V&amp;=XW_V,\ W_V\in\mathbb{R}^{m\times D_V}. \end{align}\] <p>Those are the celebrated query, key, and value matrices<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">3</a></sup>, respectively, all learnable. Typically, \(D=D_K=D_V\) makes it easier to work things out. With those matrices, we adjust attention as \(Y=\text{softmax}(QK^T)V.\)​​ Quick dimensionality check:</p> <ul> <li>The \(\text{softmax}(QK^T)\) part is a matrix with dimensions \(n\times n\), where \(n\) : number of elements in sequence.</li> <li>The matrix \(V\) has dimensions \(n\times D_V\).</li> <li>So the matrix \(Y\) has dimensions \(n\times D_V\).</li> </ul> <h2 id="scaling-self-attention">Scaling self-attention</h2> <p>While \(Y=\text{softmax}(QK^T)V\) is very close to the usual self-attention, we are missing a scaling constant. Let’s derive this here.</p> <p>Suppose that you have two \(D_K\)-dimensional vectors \(q,k\), each one with elements that have zero mean and unit variance and are independent. Then:</p> \[\text{Var}[(q,k)]=\sum_{i=1}^{D_K} \text{Var}[q_ik_i]=\sum_{i=1}^{D_K}1=D_K.\] <p>We used independence to split up \(\text{Var}[(q,k)]\). Therefore, the standard deviation of \((q,k)\) is \(\sqrt{D_K}\). This is what we need to make sure that the parts of \(QK^{T}\) have unit variance. This helps us control how big the products are, which makes learning easier. So, finally \(Y=\text{softmax}(QK^T/\sqrt{D_K})V,\) which is the usual form of dot-product attention.</p> <h3 id="does-this-make-sense">Does this make sense?</h3> <p>Zero mean and unit variance are a matter of pre-processing, but independence is not. In fact, you would hope a sequence would not have independent elements, as otherwise there is no information to use to predict the next element. I now see this scaling as a way to control the size of the terms and help with learning, but it’s important to remember this point as it not always explicitly stated<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>.</p> <h1 id="computational-costs-of-attention">Computational costs of attention</h1> <p>We need to calculate the matrix product \(QK^T\) which has computational cost \(O(nD^2)\) if we assume \(D=D_V=D_K\) and a sequence of length \(n\). Then, the matrix product \(QK^{T}V\) has cost \(O(n^2 D)\) (I’m ignoring the application of the softmax here and the scaling).</p> <p>After this part, when dealing with a transformer block, we have an MLP layer that takes as input each output from the attention layer (\(n\) of them in total). This layer has cost \(O(n^2D)\). Therefore, the total cost is \(\max\{O(nD^2), O(n^2D)\}\).</p> <p>\(D\) is fixed at the time the transformer is designed, whereas \(n\)​ is the length of the input sequence, so you can see which of the two is going to be a challenge during inference with large inputs.</p> <h5 id="footnotes">Footnotes</h5> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:3" role="doc-endnote"> <p>Bishop, C.M. and Bishop, H., 2023. <em>Deep learning: Foundations and concepts</em>. Springer Nature. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>There are so many variants of attention now: <a href="https://arxiv.org/pdf/2305.13245">grouped attention</a>, <a href="https://arxiv.org/abs/2006.16236">linearised attention</a>, etc. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:1" role="doc-endnote"> <p>Query / Key / Value is a retrieval reference; <a href="https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms">see for example on cross-validated.</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:4" role="doc-endnote"> <p>Though, the authors of <a href="https://arxiv.org/pdf/1706.03762">“Attention is all you need”</a> do mention this assumption in the celebrated “footnote 4”. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[This is a living document that I’ll extend with more information as I go. As attention is dead-center when it comes to LLMs, it’s not weird that a lot has been written about it. Here I wanted to collect some of my own questions with their answers (for easy reference).]]></summary></entry><entry><title type="html">Quick notes on the “Enhanced Cauchy Schwarz inequality and some of its statistical applications</title><link href="https://kgourgou.me/blog/2024/Quick-notes-on-the-Enhanced-Cauchy-Schwarz-inequality-and-some-of-its-statistical-applications/" rel="alternate" type="text/html" title="Quick notes on the “Enhanced Cauchy Schwarz inequality and some of its statistical applications"/><published>2024-04-11T00:00:00+00:00</published><updated>2024-04-11T00:00:00+00:00</updated><id>https://kgourgou.me/blog/2024/Quick-notes-on-the-%22Enhanced-Cauchy-Schwarz-inequality-and-some-of-its-statistical-applications</id><content type="html" xml:base="https://kgourgou.me/blog/2024/Quick-notes-on-the-Enhanced-Cauchy-Schwarz-inequality-and-some-of-its-statistical-applications/"><![CDATA[<p>I briefly read this nice paper by <a href="#Scarlatti;2024">Sergio Scarlatti</a>.</p> <p>In this, the author proves an intermediate term between the classical bounds of the CS inequality.</p> <h2 id="classical-cs-inequality">Classical CS inequality</h2> <p>First things first, for all \(x,y\) in some Hilbert space \(H\) with inner product \((.,.)\) we have:</p> \[|(x,y)|\leq \|x\|\|y\|.\] <p>The author shares a nice proof of CS that I’m not sure if I’ve seen before (but looks neat). We define the matrix \(C=C(x,y)\) with \(c_{ij}=\frac{1}{\sqrt{2}}(x_iy_j-x_jy_i),\ i,j=1,\ldots,n\). Then, its second norm is</p> \[\|C\|_2=\sqrt{\sum_{ij}c_{ij}^2}.\] <p>If we substitute the definition of \(c_{ij}\) to the above and carry out the algebra, we have \(\|C\|^2_2=\|x\|^2\|y\|^2-(x,y)^2,\) which proves CS as the norm is non-negative.</p> <h2 id="enhanced-cs">Enhanced CS</h2> <p>Now, suppose \(V\subseteq H\) is some closed subspace of \(H\). Then if \(P\) is the orthogonal projection onto \(V\) (i.e., \(PH=V\)), the author defines:</p> \[D(x,y|P):=\|Px\|\cdot \|Py\|+\|P^{\perp}x\|\cdot \|P^{\perp}y\|,\] <p>for all \(x,y\in H\) and where \(P^{\perp}\) is the projection on the orthogonal complement of \(V\). Then, \(|(x,y)|\leq D(x,y|P)\leq \|x\|\|y\|.\)</p> <p>I like inequalities with free terms! We can pick \(P\) depending on the problem and the bounds would adapt correspondingly. If \(P\) is a trivial projection to \(H\) or its complement, we recover usual CS.</p> <p><strong>Proof</strong>: It’s a short argument.</p> \[|(x,y)|=|(Px,Py)+(P^{\perp}x, P^{\perp}y)|\leq |(Px,Py)|+|(P^{\perp}x, P^{\perp}y)|\leq \|Px\|\|Py\|+\|P^{\perp}x\|\|P^{\perp}y\|,\] <p>where we used bilinearity of inner product, triangle inequality, and CS inequality for each term. Now, if \(a=\|Px\|, b=\|Py\|, c=\|P^{\perp}x\|, d=\|P^{\perp}y\|\), the author shows that</p> \[ab+cd=\sqrt{(ab+cd)^2}\leq \sqrt{(ab+cd)^2+(ac-bd)^2}=\sqrt{(a^2+c^2)(d^2+b^2)},\] <p>which is important because with the definitions of \(a,b,c,d\) above, \(a^2+c^2=\|x\|^2\) and similarly for \(y\).</p> <p>The author shows this with an appeal to algebra (as shown above), but you will notice a simpler way; this bound is just CS but applied to the vectors \(u=(a,c)\) and \(v=(b,d)\). \(\square\)</p> <h2 id="going-beyond-p">Going beyond \(P\)</h2> <p>What if we have more than one subspace? Suppose \(V, U\subseteq H\) with corresponding projections \(P,Q\), then \(x=Px+P^{\perp}x\) and similarly for \(y\), which gives</p> \[\begin{align} |(x,y)|&amp;\leq |(Px,Qy)|+|(Px, Q^{\perp}y)|+|(P^{\perp}x,Qy)|+|(P^{\perp}x, Q^{\perp}y)|\\ &amp;\leq (\|Px\|+\|P^{\perp}x\|)(\|Qy\|+\|Q^{\perp}y\|), \end{align}\] <p>Again, by bilinearity, triangle ineq., and CS.</p> <p>The last bound is not as good; setting \(P=Q\) there does not recover the previous results (note \(\|x\|\leq \|Px\|+\|P^{\perp}x\|\)). That’s because some terms would be cancelled from the first bound but are not cancelled from the second bound. For example, \((Px, P^{\perp}y)=0\) regardless of \(x,y\), and so on.</p> <h3 id="references">References</h3> <ol> <li><a name="Scarlatti;2024">Scarlatti, S., 2024. Enhanced Cauchy Schwarz inequality and some of its statistical applications. arXiv preprint arXiv:2403.13964. </a></li> </ol>]]></content><author><name></name></author><summary type="html"><![CDATA[I briefly read this nice paper by Sergio Scarlatti.]]></summary></entry><entry><title type="html">A brief analysis of automerger data, feat. SLERP and DARE-TIES LLM merging</title><link href="https://kgourgou.me/blog/2024/A-brief-analysis-of-automerger-data,-feat.-SLERP-and-DARE-TIES-LLM-merging/" rel="alternate" type="text/html" title="A brief analysis of automerger data, feat. SLERP and DARE-TIES LLM merging"/><published>2024-04-05T00:00:00+00:00</published><updated>2024-04-05T00:00:00+00:00</updated><id>https://kgourgou.me/blog/2024/A-brief-analysis-of-automerger-data,-feat.-SLERP-and-DARE-TIES-LLM-merging</id><content type="html" xml:base="https://kgourgou.me/blog/2024/A-brief-analysis-of-automerger-data,-feat.-SLERP-and-DARE-TIES-LLM-merging/"><![CDATA[<p><a href="https://huggingface.co/blog/kgourgou/a-first-look-at-automerger-data">An article I wrote</a> as a community blog post at HuggingFace.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[An article I wrote as a community blog post at HuggingFace.]]></summary></entry><entry><title type="html">Comparing what different operations do to the H_0 homology of a point cloud.</title><link href="https://kgourgou.me/blog/2024/Comparing-what-different-operations-do-to-the-$H_0$-homology-of-a-point-cloud/" rel="alternate" type="text/html" title="Comparing what different operations do to the H_0 homology of a point cloud."/><published>2024-02-16T00:00:00+00:00</published><updated>2024-02-16T00:00:00+00:00</updated><id>https://kgourgou.me/blog/2024/Comparing-what-different-operations-do-to-the-$H_0$-homology-of-a-point-cloud</id><content type="html" xml:base="https://kgourgou.me/blog/2024/Comparing-what-different-operations-do-to-the-$H_0$-homology-of-a-point-cloud/"><![CDATA[<p>I’ve recently become interested in <a href="https://jeremykun.com/2013/04/03/homology-theory-a-primer/">persistent homology</a> and using its statistics to understand how different operations change the shape of data manifolds.</p> <p>The math are really interesting (and are for another post), but I like to have some visuals that I can share with interested parties when I do presentations, so I created a notebook for this.</p> <h2 id="the-theoretical-minimum">The theoretical minimum</h2> <p>The densities below are kernel-density-estimates (aka., probability densities) of the “death times” of the \(H_0\) homology for the \(X\) point cloud (the one created with <code class="language-plaintext highlighter-rouge">make_classification</code> below). But what does “death time” mean here?</p> <p>Persistent homology (PH) is all about understanding the aspects of the shape of a manifold from sampled points (aka, a point cloud). In this note, we are looking at only one attribute that is captured by PH, the connected components of the manifold. PH looks at the point cloud at various scales, from the scale of the individual points to the scale of the entire dataset. As PH works through the different scales, it identifies when connected components get created (birth) and when they merge (death, for some of them). As every point on its own initially constitutes a connected component, the birth times are all equal to zero.</p> <p>The density plots below are tracking the death times and how those change as we manipulate the point cloud. For each case, I’m showing both the death times as they are (<code class="language-plaintext highlighter-rouge">Normalised=False</code>) and what happens if we normalise by the maximum finite persistence time. Normalising them makes the death times invariant to point cloud scaling (as you will see below).</p> <h2 id="the-plots">The plots</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N_FEAT</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">X</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nf">make_classification</span><span class="p">(</span>
    <span class="n">n_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">n_features</span><span class="o">=</span><span class="n">N_FEAT</span><span class="p">,</span>
    <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">n_informative</span><span class="o">=</span><span class="n">N_FEAT</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">n_clusters_per_class</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">class_sep</span><span class="o">=</span><span class="mf">1.0</span>
<span class="p">)</span>

<span class="c1"># change X to have mean 2 and std 3
</span><span class="n">X</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">X</span>
</code></pre></div></div> <p><img src="http://kgourgou.me/assets/output_files/output_3_0.png" alt="png"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># contraction mapping
</span><span class="n">Xcontr</span> <span class="o">=</span> <span class="n">X</span> <span class="o">/</span> <span class="mi">2</span>
</code></pre></div></div> <p><img src="http://kgourgou.me/assets/output_files/output_4_0.png" alt="png"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># expansion mapping
</span><span class="n">Xexp</span> <span class="o">=</span> <span class="n">X</span> <span class="o">*</span> <span class="mi">2</span>
</code></pre></div></div> <p><img src="http://kgourgou.me/assets/output_files/output_5_0.png" alt="png"/></p> <p>Things are as expected up to this point. A few more interesting operations follow.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate a random affine contraction matrix A
</span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">N_FEAT</span><span class="p">,</span> <span class="n">N_FEAT</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">A</span>  <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="o">+</span><span class="mf">1e-10</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">N_FEAT</span><span class="p">)</span>

<span class="n">Xaff</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">A</span> <span class="o">+</span> <span class="n">b</span>
</code></pre></div></div> <p><img src="http://kgourgou.me/assets/output_files/output_6_1.png" alt="png"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># map to a lower dimension
</span><span class="n">Xlow</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
</code></pre></div></div> <p><img src="http://kgourgou.me/assets/output_files/output_7_0.png" alt="png"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># map to lower dimensions with a random affine map
</span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">N_FEAT</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>


<span class="n">Xaff</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">A</span> <span class="o">+</span> <span class="n">b</span>
</code></pre></div></div> <p><img src="http://kgourgou.me/assets/output_files/output_8_1.png" alt="png"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># same affine transformation but with a relu function applied to the output
</span>
<span class="n">Xaff_relu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">X</span> <span class="o">@</span> <span class="n">A</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div> <p><img src="http://kgourgou.me/assets/output_files/output_9_0.png" alt="png"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># two layer neural network with relu activation
</span><span class="n">N_OUTPUT</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">A1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">N_FEAT</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="n">A2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">N_OUTPUT</span><span class="p">)</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">N_OUTPUT</span><span class="p">)</span>

<span class="n">Xnn</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">A1</span> <span class="o">+</span> <span class="n">b1</span>
<span class="n">Xnn</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xnn</span><span class="p">)</span>
<span class="n">Xnn</span> <span class="o">=</span> <span class="n">Xnn</span> <span class="o">@</span> <span class="n">A2</span> <span class="o">+</span> <span class="n">b2</span>

</code></pre></div></div> <p><img src="http://kgourgou.me/assets/output_files/output_10_1.png" alt="png"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># layernorm
</span>
<span class="n">Xlayernorm</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span> <span class="o">/</span> <span class="n">X</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

</code></pre></div></div> <p><img src="http://kgourgou.me/assets/output_files/output_11_0.png" alt="png"/></p>]]></content><author><name></name></author><category term="blog"/><summary type="html"><![CDATA[I’ve recently become interested in persistent homology and using its statistics to understand how different operations change the shape of data manifolds.]]></summary></entry><entry><title type="html">SetFit and Integrated Gradients</title><link href="https://kgourgou.me/blog/2023/SetFit-and-Integrated-Gradients/" rel="alternate" type="text/html" title="SetFit and Integrated Gradients"/><published>2023-07-30T00:00:00+00:00</published><updated>2023-07-30T00:00:00+00:00</updated><id>https://kgourgou.me/blog/2023/SetFit-and-Integrated-Gradients</id><content type="html" xml:base="https://kgourgou.me/blog/2023/SetFit-and-Integrated-Gradients/"><![CDATA[<p>I’m a fan of both SetFit and integrated gradients, so I wrote this tiny library to combine them. The code is available <a href="https://github.com/kgourgou/setfit-integrated-gradients">here</a> under MIT license for further hacking by others. I’m fixing bugs, but otherwise not actively maintaining it.</p> <h2 id="what-is-setfit-">What is SetFit ?</h2> <p>It’s a <a href="https://huggingface.co/blog/setfit">language-model-tuning paradigm </a> for few-shot learning with language models without using prompts. It relies on contrastive learning and the authors published a really nice library that makes the method plug &amp; play with sentence-transformers.</p> <p>I have been contributing some time to that library on Github as well.</p> <h2 id="what-are-integrated-gradients-">What are integrated gradients ?</h2> <p><a href="https://www.tensorflow.org/tutorials/interpretability/integrated_gradients">Integrated gradients (IG)</a> are a method for attributing the output of a neural network to its inputs. It was first developed to explain the output of image classifiers, but it can be used for any model that takes a vector as input.</p> <p>It occured to me by building this that there are various places one could perturb to apply IG and that the perturbation path probably also matters a lot.</p>]]></content><author><name></name></author><category term="blog"/><summary type="html"><![CDATA[I’m a fan of both SetFit and integrated gradients, so I wrote this tiny library to combine them. The code is available here under MIT license for further hacking by others. I’m fixing bugs, but otherwise not actively maintaining it.]]></summary></entry><entry><title type="html">Control variates and hypothesis testing</title><link href="https://kgourgou.me/blog/2023/Control-variates-and-hypothesis-testing/" rel="alternate" type="text/html" title="Control variates and hypothesis testing"/><published>2023-01-01T00:00:00+00:00</published><updated>2023-01-01T00:00:00+00:00</updated><id>https://kgourgou.me/blog/2023/Control-variates-and-hypothesis-testing</id><content type="html" xml:base="https://kgourgou.me/blog/2023/Control-variates-and-hypothesis-testing/"><![CDATA[<p>I haven’t looked at this in a while, so thought I would revise a bit.</p> <p>To start with, I have a variable, say, \(Y\sim P\). Given data from this variable, \(y_i\), we can estimate the mean</p> \[E[Y]\approx S_N=\frac{1}{N}\sum_{i=1}^{N}y_i.\] <p>The estimator we all know and use. It’s unbiased, but it may have large variance, which means that for a fixed N, most random sums could fall away from \(E[Y]\). What can we do to improve on this?</p> <p>One idea is to introduce a new variable, \(X\). Then, we adjust the data as</p> \[y'_i=y_i+a(x_i-E[x_i]),\] <p>where \(a\) is some parameter that we can pick later. What’s the advantage of this? First, this adjustment doesn’t change the mean: \(E[Y']=E[Y]+a(E[X]-E[X])=E[Y].\) So an estimator based on \(Y'\) is unbiased. Not bad.</p> <p>What is the variance of this new estimator?</p> \[\mathrm{Var}[Y'] = \mathrm{Var}[Y] + a^2\mathrm{Var}[X] + 2\mathrm{Cov}[Y,aX].\] <p>Now the first two terms are non-negative. However, the last one can be negative as long as \(X\) and \(Y\) have negative correlation. For instance, picking \(X=-Y\),</p> \[\mathrm{Var}[Y'] = \mathrm{Var}[Y] + \mathrm{Var}[Y] - 2\mathrm{Cov}[Y,Y] = 0,\] <p>which is the best possible variance. Realistically, we are back to where we started if we set \(X=-Y\), however if we have other variables that are close to \(-Y\), then this idea can get quite useful (and dramatically reduce variance).</p> <h2 id="in-hypothesis-testing">In hypothesis testing</h2> <p>Why would all this be useful for hypothesis testing?</p> <p>To conduct it, we first split the population to two groups, traditionally called the “treatment” and “control”. We have intervened on the treatment group in some way, say via a variable \(Z\), and we wish to understand whether the magnitude of \(E[Y_T-Y_C]\) (this is also called the average treatment effect (ATE)) is due to \(Z\) or random chance.</p> <p>Suppose that the difference is indeed due to \(Z\). If we use data to estimate \(E[Y_{T}-Y_{C}]\), the estimator can have variance and that will affect how small an effect we can confidently separate from random chance.</p> <p>We are now thinking about the “<em>power</em> “ of a hypothesis test, and there are at least three ways to improve our situation from here:</p> <ol> <li>Give up on the small effect size and go for a larger one.</li> <li>Get more \(Y_T, Y_C\) data for the estimator, i.e., make the groups larger.</li> <li>Use other covariates, \(X\), to reduce the variance of the ATE estimator. Only covariates that are independent of the way the groups are split can be used, for example pre-experiment data.</li> </ol> <p>Now we can discuss two ways to increase the power of the test by using 3.</p> <h3 id="cuped-and-cupac">CUPED and CUPAC</h3> <p>You can find implementations of <a href="https://github.com/kgourgou/cupac_cuped_control_variates/blob/main/cuped_cupac.ipynb">CUPED and CUPAC in this notebook</a>.</p> <p>CUPED stands for “Controlled-Experiment using Pre-experiment data”; see Deng, Xu, Kohavi, Walker, 2013.</p> <p>At its core, CUPED is a proposal for how to use pre-experiment data with control variates to reduce the variance of the ATE estimator. The authors propose using any covariates that we have before the experiment took place, \(X_i\), \(i=1,\ldots, n\), as well as past values of \(Y\) to fit a linear model: \(\hat{Y}=X\beta +\epsilon\)</p> <p>Then, we can use the linear model to get predictions for the \(Y\) variables in the control and experiment groups and adjust the true values as \(Y'_T=Y_{T}-(\hat{Y}_T-E[\hat{Y}_T]),\) and similarly for \(Y_C\). Instead of the original ATE, we will then construct an estimator for \(E[Y'_T-Y'_C]\) which, because of our control variates method, will have smaller variance!</p> <p>CUPAC, aka., Control Using Predictions As Covariates, introduced by DoorDash’s engineering, <a href="https://doordash.engineering/2020/06/08/improving-experimental-power-through-control-using-predictions-as-covariate-cupac/">Li, Tang, and Bauman</a>, takes this one step further: there’s nothing special about using a linear model. One can use a more expressive ML model, get a closer fit to \(Y\), and reduce variance further.</p>]]></content><author><name></name></author><category term="blog"/><summary type="html"><![CDATA[I haven’t looked at this in a while, so thought I would revise a bit.]]></summary></entry><entry><title type="html">Stein and the Normal Distribution</title><link href="https://kgourgou.me/blog/2021/Stein-and-the-Normal-Distribution/" rel="alternate" type="text/html" title="Stein and the Normal Distribution"/><published>2021-01-22T00:00:00+00:00</published><updated>2021-01-22T00:00:00+00:00</updated><id>https://kgourgou.me/blog/2021/Stein-and-the-Normal-Distribution</id><content type="html" xml:base="https://kgourgou.me/blog/2021/Stein-and-the-Normal-Distribution/"><![CDATA[<p>Hello and happy 2021!</p> <p>Inspired from this <a href="https://twitter.com/docmilanfar/status/1312936010393640961?s=20">tweet</a>, I wanted to understand the basics of Stein’s characterization of the Normal distribution.</p> <p>With Stein’s idea, we can identify the distribution of a random variable by checking that it satisfies some condition in expectation. For example, for the standard normal, we have that \(X\sim N(0,1)\) if and only if for all \(f\) with \(E[f']&lt;\infty\) we have:</p> \[\mathbb{E}[xf(x)-f'(x)]=0.\] <p>The operator \(Af:=xf-f'\) is then called the Stein operator and we can rewrite the result with this operator as: \(\mathbb{E}_{P}[Af]=0\) for all \(f\in C^1_b\) iff \(P\) is \(N(0,1)\).</p> <p>This operator is not unique as we can always add P-measure zero parts; see \(Bf:=xf-f'+x\) which satisfies</p> \[\mathbb{E}[Bf]=\mathbb{E}[Af]+\mathbb{E}[x]=\mathbb{E}[Af].\] <p>How can we show \(A\) characterises the standard Normal? A key identity is that the probability density function (PDF) of the \(N(0,1)\) satisfies \(P'+xP=0\). So, if \(P\) is indeed the PDF of the standard normal, then applying integration by parts to \(E_{P}[f']\) and using the differential equation gives us Stein’s formula.</p> <p>Now, if \(P\) is the PDF of any other distribution and it satisfies Stein’s formula for all \(f\in C^1_b\), then integration by parts on \(E_{P}[f']\) leads us back to the \(P'+xP=0\). That ODE is separable with solution \(P\propto \exp(-x^2/2)\), which, up to the normalisation, is the PDF of the standard normal!</p> <p>This strategy of deriving an ODE for the density function, getting its weak form by multiplying with a smooth function \(f\) and integrating can be repeated to get <a href="https://en.wikipedia.org/wiki/Stein%27s_method#The_Stein_operator">Stein operators</a> for other distributions, e.g., the exponential, etc.</p>]]></content><author><name></name></author><category term="blog"/><summary type="html"><![CDATA[Hello and happy 2021!]]></summary></entry><entry><title type="html">A computable lower bound for the KL from Hammersley-Chapman-Robbins inequality</title><link href="https://kgourgou.me/blog/2020/A-computable-lower-bound-for-the-KL-from-Hammersley-Chapman-Robbins-inequality/" rel="alternate" type="text/html" title="A computable lower bound for the KL from Hammersley-Chapman-Robbins inequality"/><published>2020-12-28T00:00:00+00:00</published><updated>2020-12-28T00:00:00+00:00</updated><id>https://kgourgou.me/blog/2020/A-computable-lower-bound-for-the-KL-from-Hammersley-Chapman-Robbins-inequality</id><content type="html" xml:base="https://kgourgou.me/blog/2020/A-computable-lower-bound-for-the-KL-from-Hammersley-Chapman-Robbins-inequality/"><![CDATA[<p>I first read of this bound from:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Nishiyama, T., 2019. A New Lower Bound for Kullback-Leibler Divergence Based on Hammersley-Chapman-Robbins Bound. arXiv:1907.00288 [cs, math, stat].
</code></pre></div></div> <p>The following notes are not very polished, but present the general idea. I hope they are useful.</p> <p>The strategy of the paper is quite nice; first, Nishiyama shows that if we have two distributions \(P,Q\) and define the mixture \(R_t(x):=P(x)+t(Q(x)-P(x)), t\in [0,1]\), then:</p> \[\frac{d}{dt}D_a(P|R_t)=\frac{1-a}{t}D_a(P|R_t)+\frac{1+a}{t}D_{a+1}(P|R_t),\] <p>for any \(a\) and \(D_a\) being the alpha-divergence. Setting \(a=1\) leaves us with the KL divergence and \(\chi^2\), which recovers the nice identity:</p> \[\frac{d}{dt}KL(P|R_t)=\frac{1}{t}\chi^2(P|R_t).\] <p>Now, fix a function \(f\) for which \(E_Q, E_P, V_P, V_{R_t}\) (expectations and variances) are finite and \(V_{R_t}&gt;0\) for all \(t\in [0,1]\). Then, applying the HCR inequality gives:</p> \[\frac{d}{dt}KL(P|R_t)\geq \frac{(E_{R_t}-E_P)^2}{tV_{R_t}}.\] <p>Integrating the above in \([0,1]\) gives the result of the paper as \(\int_0^1 KL(P|R_t)dt=KL(P|Q).\)</p> <p>We can also show that</p> \[KL(P|Q)=\int_0^1 \frac{\chi^2(P|R_t)}{t}dt=\int_0^1 t\int_{\Omega}\frac{(Q-P)^2}{P+t(Q-P)}dxdt.\] <p>Assuming everything exists, we can exchange the integrals (ala. Fubini) and then expand the \(t\) function around \(t=1\) to introduce the chi-square divergence:</p> \[KL(P|Q)=\chi^2(P|Q)+...\] <h2 id="actual-lower-bound">Actual lower bound</h2> <p>The lower bound is:</p> \[KL(P|Q)\geq \int_0^1\frac{(E_{R_t}-E_P)^2}{tV_{R_t}}dt,\] <p>where the integral depends on \(E_P, E_Q, V_P, V_Q\) and can be computed analytically and written as a sum of logarithms (by using partial fractions).</p> <h1 id="tighter-lower-bounds">Tighter lower-bounds</h1> <p>Starting from the equality:</p> \[KL(P|Q)=\int_0^1 \frac{\chi^2(P|R_t)}{t}dt\] <p>we can derive tighter lower bounds for the KL. First, we write the variational representation of \(\chi^2\)</p> \[\chi^2(P|R_t)=\sup_{h}\left \{ 2E_P[h]-E_Q[h^2]-1\right \}.\] <p>Suppressing the family of functions leads to lower bounds of chi-square and thus to lower bounds of the KL. For example, the HCR bound can be derived by considering first degree polynomials.</p>]]></content><author><name></name></author><category term="blog"/><summary type="html"><![CDATA[I first read of this bound from:]]></summary></entry><entry><title type="html">Rediscovering a function from samples</title><link href="https://kgourgou.me/blog/2020/Rediscovering-a-function-from-samples/" rel="alternate" type="text/html" title="Rediscovering a function from samples"/><published>2020-12-28T00:00:00+00:00</published><updated>2020-12-28T00:00:00+00:00</updated><id>https://kgourgou.me/blog/2020/Rediscovering-a-function-from-samples</id><content type="html" xml:base="https://kgourgou.me/blog/2020/Rediscovering-a-function-from-samples/"><![CDATA[<p>A friend shared this nice problem with me. Suppose you have a fixed function, \(f\), and a family of probability distributions, defined by, say, prob. density functions, \(P_t\), \(t \in A\). If we know \(E_t=E_{P_t}[f]\) for every \(t\in A\), can we recover \(f\)?</p> <p>Clearly the answer depends on both how rigid \(f\) is and the family \(P_t\). We can cast this problem as a functional analysis problem by defining \(k(x,t)=P_t(x)\) to be a kernel and the expectation to be an integral transform. Then the question becomes: is there an inverse kernel, say, \(k^{-1}(x,t)\), such that</p> \[\int k^{-1}(x,t)E(t)dt=f(x)?\] <p>When does that exist and when is it unique? Hints can be taken from the Laplace transform, i.e., \(P_t(x)\propto e^{-tx}\) - up to a normalizing constant this is the just the exponential distribution. In general, this can be a hard problem though.</p> <h2 id="fredholm-equations">Fredholm equations</h2> <p>If we know \(E(t)=E_{P_t}[f]\) for every \(t\in A\), can we recover \(f\)? Formally, we have the equation:</p> \[E(t)=\int k(x,t)f(x)dx,\] <p>with appropriate limits for the integral. This equation is called a <a href="https://en.wikipedia.org/wiki/Fredholm_integral_equation">“Fredholm Equation of the first kind”</a> and is closely studied in functional analysis and signal processing.</p> <h2 id="practical-stuff">Practical stuff</h2> <p>If we assume the existence of an inverse kernel, how can we approximate it? One idea — which is also kind of a standard approach — is to fix a set of orthonormal basis functions, describe everything in terms of them, and then resolve them to arrive to a linear algebra problem.</p>]]></content><author><name></name></author><category term="blog"/><summary type="html"><![CDATA[A friend shared this nice problem with me. Suppose you have a fixed function, \(f\), and a family of probability distributions, defined by, say, prob. density functions, \(P_t\), \(t \in A\). If we know \(E_t=E_{P_t}[f]\) for every \(t\in A\), can we recover \(f\)?]]></summary></entry></feed>